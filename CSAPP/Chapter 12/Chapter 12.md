https://www.cs.cmu.edu/afs/cs/academic/class/18213-m25/www/lectures/22-concprog.pdf
https://www.youtube.com/watch?v=jKWfQfZkQ-w&list=PL22J-I2Pi-Gf0s1CGDVtt4vuvlyjLxfem&index=23
So as soon as we have multiple flow accessing shared resources all kinds of bad things can happen in your program. 

Classical problem classes of concurrent programs:
- Races: outcome depends on arbitrary scheduling decisions elsewhere in the system
	- Example: who gets the last seat on the airplane?
	- Job-list - runs and finish before the parents has chance to add the child in the list
	
- Deadlock: improper resource allocation prevents forward progress
	- Example: traffic gridlock
	- printf - interrupted by signal handler - printf
		- Printf acquires a terminal lock but the second printf won't be able to get it because the printf and the main routine has it
		- So now your signal handler the printf and the signal handler is waiting for the event that will never occur
		- ![[Pasted image 20251103101607.png]]
- Livelock / Starvation / Fairness: external events and/or system scheduling decisions can prevent sub-task progress
			- Example: people always jump in front of you in line

Iterative servers
- Iterative servers process on request at a time.
	- ![[Pasted image 20251103101819.png]]
	- ![[Pasted image 20251103101840.png]]
	- Where Does Second Client Actually Block?
		- Second client attempts to connect to iterative server
			- Due to TCP Buffering:
				- Call to connect returns
					- Even though connection not yet accepted
					- Server side TCP manager queues request
				- Call to rio_writen returns
					- Server side TCP manager buffers input data
				- Call to rio_readlineb blocks!
					- Server hasn’t written anything for it to read yet.
- Fundamental Flaw of Iterative Servers
	- ![[Pasted image 20251103102641.png]]
	- Client 1:
		- User goes out to lunch
		- Client 1 blocks waiting for user to type in data
	- Server
		- Server blocks waiting for data from Client 1
	- Client 2:
		- Client 2 blocks waiting to read from server
	
	- We are in the untenable situation - where one client has sort of totally affected all of the other clients in the system and none of the other clients can get service.
		- Solution: use concurrent servers instead
		- Concurrent servers use multiple concurrent flows to serve multiple clients at the same time
- Approaches for Writing Concurrent Servers
	- Allow server to handle multiple clients concurrently
		- 1. Process-based
			- Kernel automatically interleaves multiple logical flows
			- Each flow has its own private address space
				- each flow is independent and controlled by kernel
		- 2. Event-based
			- Programmer manually interleaves multiple logical flows
			- All flows share the same address space
			- Uses technique called I/O multiplexing
				- The user and programmer creates this flow and manually interleaves this flows.
		- 3. Thread-based
			- Kernel automatically interleaves multiple logical flows
			- Each flow shares the same address space
			- Hybrid of process-based and event-based
				- Each of these flows are implemented by thread

- Approach #1: Process-based Servers
	- Spawn separate process for each client
		- ![[Pasted image 20251103103417.png]]
	- Process-Based concurrent echo server
		- argv  - pass in the port number that we want this server to listen on
		- sockaddr_storage - protocol independent, big enough to handle IPv4, IPv6
		- listenfd - create listening descriptor
		- ![[Pasted image 20251103104453.png]]
		- only the child server needs the connfd
		- ![[Pasted image 20251103104527.png]]
- Concurrent Server: accept Illustrated
	- 1. Server blocks in accept, waiting for connection request on listening descriptor listenfd
	- 2. Client makes connection request by calling connect
	- 3. Server returns connfd from accept. Forks child to handle client. Connection is now established between clientfd and connfd
	- ![[Pasted image 20251103104603.png]]
- Process-based server execution model
	- ![[Pasted image 20251103104846.png]]
	- Each client handled by independent child process
	- No shared state between them
	- Both parent & child have copies of listenfd and connfd
		- Parent must close connfd
		- Child should close listenfd
- Issues with Process-based Servers
	- Listening server process must reap zombie children
		- to avoid fatal memory leak
	- Parent process must close its copy of connfd
		- Kernel keeps reference count for each socket/open file
		- After fork, refcnt(connfd) = 2
		- Connection will not be closed until refcnt(connfd) = 0
- Pros and Cons of Process-based Servers
	- + Handle multiple connections concurrently.
	- + Clean sharing model
		- descriptors (no) - separate copies of descriptors
		- file tables (yes)
		- global variable (no)
	- + Simple and straightforward.
	- – Additional overhead for process control.
	- – Nontrivial to share data between processes.
		- requires IPC(interprocess communication) mechanisms
			- FIFO's (named pipes)

- Approach #2: Event-based Servers
	- Server maintains set of active connections
		- Array of connfd’s and listenfd's
	- Repeat:
		- Determine which descriptors (connfd’s or listenfd) have pending inputs
			- e.g., using select or epoll function
			- arrival of pending input is an event
		- If listenfd has input, then accept connection
			- and add new connfd to array
		- Service all connfd’s with pending inputs
	- I/O Multiplexed Event Processing
		- 
		- left - we record the descriptor number for each of those connected
		- ![[Pasted image 20251103110456.png]]
		- Pros and Cons of Event-based Servers
			- + One logical control flow and address space.
			- + Can single-step with a debugger.
			- + No process or thread control overhead.
				- Just the list of confd's
				- Design of choice for high-performance Web servers and search engines. e.g., Node.js, nginx, Tornado
			- – Significantly more complex to code than process-based or thread-based design
				- figure out how much work that we will do in response to an event.
				- ex. web server
					- you get input on one of your connected file descriptor
						- Simplest thing to do would be to then assume to read the entire http request
							- And not return until we read the entire request - the amount of work that you do in response to an event is coarse-grained
								- a lot of instruction done.
			- – Hard to provide fine-grained concurrency. 
				- ▪ E.g., how to deal with partial HTTP request headers
			- – Cannot take advantage of multi-core. 
				- ▪ Single thread of control
- Approach #3: Thread-based Servers
	- Traditional View of a Process
		- Process = process context + code, data, and stack
			- context - data structure in the kernel, data that the kernel keeps about the process
			- And the private address space = code, data, stack
			- ![[Pasted image 20251103111914.png]]
	- Alternate View of a Process
		- Process = thread + code, data, and kernel context
		- ![[Pasted image 20251103112057.png]]
		- Thread context = program context
	- A Process With Multiple Threads
		- ![[Pasted image 20251103112800.png]]
		- Multiple threads can be associated with a process
			- Each thread has its own logical control flow
			- Each thread shares the same code, data, and kernel context
			- Each thread has its own stack for local variables
				- but not protected from other threads
			- Each thread has its own thread id (TID)
		- Kernel can create each of these threads as a separate flow of control and schedules similar with the process
		- But the difference, when the kernel wants to context switch from one thread to another, there's not that much information that has to be saved and restored. It's just small amount of data.
			- very low overhead
	- Logical View of Threads
		- Threads associated with process form a pool of peers
			- Unlike processes which form a tree hierarchy
			- ![[Pasted image 20251103112911.png]]
	- Concurrent Threads
		- Two threads are concurrent if their flows overlap in time
		- Otherwise, they are sequential
		- Examples:
			- Concurrent: A & B, A&C
			- Sequential: B & C
			- ![[Pasted image 20251103113051.png]]
		- Single Core Processor
			- Simulate parallelism by time slicing
		- Multi-Core Processor
			- Can have true parallelism
		- ![[Pasted image 20251103113132.png]]
- Threads vs. Processes
	- How threads and processes are similar
		- Each has its own logical control flow
		- Each can run concurrently with others (possibly on different cores)
		- Each is context switched
	- How threads and processes are different
		- Threads share all code and data (except (unprotected) local stacks)
			- Processes (typically) do not - they have their own address spaces
		- Threads are somewhat less expensive than processes
			- Process control (creating and reaping) twice as expensive as thread control
				- Linux numbers
					- – ~20K cycles to create and reap a process
					- – ~10K cycles (or less) to create and reap a thread
- Posix Threads (Pthreads) Interface
	- Pthreads: Standard interface for ~60 functions that manipulate threads from C programs
		- Creating and reaping threads
			- pthread_create() === fork, but doesn't create hierarchy
			- pthread_join() === wait
		- Determining your thread ID
			- pthread_self() === getpid
		- Terminating threads
			- pthread_cancel() === kill
			- pthread_exit()
			- exit() (terminates all threads)
			- return (terminates current thread)
		- Synchronizing access to shared variables
			- pthread_mutex_init
			- pthread_mutex_[un]lock
- Pthreads "hello, world" program
	- ![[Pasted image 20251103114055.png]]
	- ![[Pasted image 20251103114357.png]]
- Thread-Based Concurrent Echo Server
	- ![[Pasted image 20251103114625.png]]
	- 
	- ![[Pasted image 20251103114908.png]]
	- vargp - pointer to the connected file descriptor
	- detach - when it dies the kernel automatically handles the data.
	- free - malloc that's created in the main thread
	- Run thread in “detached” mode.
		- Runs independently of other threads
		- Reaped automatically (by kernel) when it terminates
	- Free storage allocated to hold connfd.
	- Close connfd (important!)
- 