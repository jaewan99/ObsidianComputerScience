https://www.cs.cmu.edu/afs/cs/academic/class/18213-m25/www/lectures/22-concprog.pdf
https://www.youtube.com/watch?v=jKWfQfZkQ-w&list=PL22J-I2Pi-Gf0s1CGDVtt4vuvlyjLxfem&index=23
So as soon as we have multiple flow accessing shared resources all kinds of bad things can happen in your program. 

Classical problem classes of concurrent programs:
- Races: outcome depends on arbitrary scheduling decisions elsewhere in the system
	- Example: who gets the last seat on the airplane?
	- Job-list - runs and finish before the parents has chance to add the child in the list
	
- Deadlock: improper resource allocation prevents forward progress
	- Example: traffic gridlock
	- printf - interrupted by signal handler - printf
		- Printf acquires a terminal lock but the second printf won't be able to get it because the printf and the main routine has it
		- So now your signal handler the printf and the signal handler is waiting for the event that will never occur
		- ![[Pasted image 20251103101607.png]]
- Livelock / Starvation / Fairness: external events and/or system scheduling decisions can prevent sub-task progress
			- Example: people always jump in front of you in line

Iterative servers
- Iterative servers process on request at a time.
	- ![[Pasted image 20251103101819.png]]
	- ![[Pasted image 20251103101840.png]]
	- Where Does Second Client Actually Block?
		- Second client attempts to connect to iterative server
			- Due to TCP Buffering:
				- Call to connect returns
					- Even though connection not yet accepted
					- Server side TCP manager queues request
				- Call to rio_writen returns
					- Server side TCP manager buffers input data
				- Call to rio_readlineb blocks!
					- Server hasn’t written anything for it to read yet.
- Fundamental Flaw of Iterative Servers
	- ![[Pasted image 20251103102641.png]]
	- Client 1:
		- User goes out to lunch
		- Client 1 blocks waiting for user to type in data
	- Server
		- Server blocks waiting for data from Client 1
	- Client 2:
		- Client 2 blocks waiting to read from server
	
	- We are in the untenable situation - where one client has sort of totally affected all of the other clients in the system and none of the other clients can get service.
		- Solution: use concurrent servers instead
		- Concurrent servers use multiple concurrent flows to serve multiple clients at the same time
- Approaches for Writing Concurrent Servers
	- Allow server to handle multiple clients concurrently
		- 1. Process-based
			- Kernel automatically interleaves multiple logical flows
			- Each flow has its own private address space
				- each flow is independent and controlled by kernel
		- 2. Event-based
			- Programmer manually interleaves multiple logical flows
			- All flows share the same address space
			- Uses technique called I/O multiplexing
				- The user and programmer creates this flow and manually interleaves this flows.
		- 3. Thread-based
			- Kernel automatically interleaves multiple logical flows
			- Each flow shares the same address space
			- Hybrid of process-based and event-based
				- Each of these flows are implemented by thread

- Approach #1: Process-based Servers
	- Spawn separate process for each client
		- ![[Pasted image 20251103103417.png]]
	- Process-Based concurrent echo server
		- argv  - pass in the port number that we want this server to listen on
		- sockaddr_storage - protocol independent, big enough to handle IPv4, IPv6
		- listenfd - create listening descriptor
		- ![[Pasted image 20251103104453.png]]
		- only the child server needs the connfd
		- ![[Pasted image 20251103104527.png]]
- Concurrent Server: accept Illustrated
	- 1. Server blocks in accept, waiting for connection request on listening descriptor listenfd
	- 2. Client makes connection request by calling connect
	- 3. Server returns connfd from accept. Forks child to handle client. Connection is now established between clientfd and connfd
	- ![[Pasted image 20251103104603.png]]
- Process-based server execution model
	- ![[Pasted image 20251103104846.png]]
	- Each client handled by independent child process
	- No shared state between them
	- Both parent & child have copies of listenfd and connfd
		- Parent must close connfd
		- Child should close listenfd
- Issues with Process-based Servers
	- Listening server process must reap zombie children
		- to avoid fatal memory leak
	- Parent process must close its copy of connfd
		- Kernel keeps reference count for each socket/open file
		- After fork, refcnt(connfd) = 2
		- Connection will not be closed until refcnt(connfd) = 0
- Pros and Cons of Process-based Servers
	- + Handle multiple connections concurrently.
	- + Clean sharing model
		- descriptors (no) - separate copies of descriptors
		- file tables (yes)
		- global variable (no)
	- + Simple and straightforward.
	- – Additional overhead for process control.
	- – Nontrivial to share data between processes.
		- requires IPC(interprocess communication) mechanisms
			- FIFO's (named pipes)

- Approach #2: Event-based Servers
	- Server maintains set of active connections
		- Array of connfd’s and listenfd's
	- Repeat:
		- Determine which descriptors (connfd’s or listenfd) have pending inputs
			- e.g., using select or epoll function
			- arrival of pending input is an event
		- If listenfd has input, then accept connection
			- and add new connfd to array
		- Service all connfd’s with pending inputs
	- I/O Multiplexed Event Processing
		- 
		- left - we record the descriptor number for each of those connected
		- ![[Pasted image 20251103110456.png]]
		- Pros and Cons of Event-based Servers
			- + One logical control flow and address space.
			- + Can single-step with a debugger.
			- + No process or thread control overhead.
				- Just the list of confd's
				- Design of choice for high-performance Web servers and search engines. e.g., Node.js, nginx, Tornado
			- – Significantly more complex to code than process-based or thread-based design
				- figure out how much work that we will do in response to an event.
				- ex. web server
					- you get input on one of your connected file descriptor
						- Simplest thing to do would be to then assume to read the entire http request
							- And not return until we read the entire request - the amount of work that you do in response to an event is coarse-grained
								- a lot of instruction done.
			- – Hard to provide fine-grained concurrency. 
				- ▪ E.g., how to deal with partial HTTP request headers
			- – Cannot take advantage of multi-core. 
				- ▪ Single thread of control
- 